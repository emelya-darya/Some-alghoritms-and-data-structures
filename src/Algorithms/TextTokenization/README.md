# Лексический анализ / токенизация (англ. Lexical analysis / tokenizing)

__Лексический анализ__ — процесс аналитического разбора входной последовательности символов на распознанные группы — лексемы — с целью получения на выходе идентифицированных последовательностей, называемых «токенами». Лексемы могут быть словами, символами или даже подсловами в зависимости от того, какой алгоритм разделения используется.

***Лексема*** — это отдельная идентифицируемая последовательность символов.

***Токен*** — это объект, описывающий лексему. Содержит значение (фактические символы описываемой лексемы) и набор дополнительных атрибутов.

Как правило, лексический анализ производится с точки зрения определённого формального языка или набора языков. Язык, а точнее, его грамматика, задаёт определённый набор лексем, которые могут встретиться во входной последовательности.

В простых случаях понятия «лексема» и «токен» идентичны, но более сложные токенизаторы дополнительно классифицируют лексемы по различным типам («идентификатор», «оператор», «часть речи» и т. п.). 

Лексический анализ используется в компиляторах и интерпретаторах исходного кода языков программирования, и в различных парсерах слов естественных языков. Цель токенизации обычно состоит в том, чтобы подготовить входную последовательность для другой программы, например для грамматического анализатора.

<br/>

### Обработка текстов на естественном языке (англ. Natural Language Processing, NLP)

***NLP*** — одно из направлений искуственного интеллекта, которое работает с анализом, пониманием и генерацией живых языков, для того, чтобы взаимодействовать с компьютерами и устно, и письменно, используя естественные языки вместо компьютерных.

При работе с естественными языками входные данные перед сегментацией на лексемы, проходят этапы предварительной обработки (эти этапы могут варьироваться в зависимости от задачи): 
- приведение к нижнему регистру; 
- удаление стоп-слов, знаков препинания (стоп-слова — это часто используемые слова, которые не вносят никакой дополнительной информации в текст, например «the», «is», «a»);
- ***стеммизация*** (процесс приведения слова к его корню/основе);
- ***лемматизация*** (лемматизация похожа на стеммизацию в том, что она приводит слово к его начальной форме, но с одним отличием: в данном случае корень слова будет существующим в языке словом, например слово «caring» прекратится в «care», а не «car», как в стеммизации).
_____

#### Методы токенизации при обработке текстов на естественном языке
Сегментацию входных текстовых данных на отдельные лексемы можно проводить на основе слов, символов или подслов:

<br/>

1. __Токенизация на основе слов (word-based tokenization)__
Разделить текст можно разными способами. Например, мы можем использовать пробельные символы, чтобы разделить текст на слова. Существуют также разновидности токенизаторов слов, которые содержат дополнительные правила для пунктуации. 
Используя такой токенизатор, мы можем получить довольно большие «словари». Если мы хотим полностью покрыть язык с помощью токенизатора, основанного на словах, нам понадобится идентификатор для каждого слова в языке, что приведет к созданию огромного количества токенов (например, в английском языке более 500 000 слов). 
Еще одним минусом такого подхода является то, что даже несмотря на внушительный размер словаря, всегда можно столкнуться с получением неизвестного слова, что ведет к потере информации. 

<br/>

2. __Токенизация на основе символов (character-based tokenization)__
При таком подходе входной текст разбивается на символы, а не на слова. Словарь при этом будет намного меньше (~200 символов). Однако, поскольку представление теперь основано на символах, а не на словах, можно утверждать, что интуитивно оно менее осмысленно: каждый символ сам по себе мало что значит, в то время как в случае со словами это не так.
Еще один момент, который следует учитывать, — это то, что основная программа после такой сегментации будет получать на вход очень большое количество токенов: если при использовании токенизатора, основанного на словах, одному слову будет соответствовать один токен, то при сегментации на символы количество токенов будет равно количеству символов в слове.

<br/>

<small>Чтобы получить лучшее из обоих подходов, мы можем использовать третью технику, которая объединяет эти два подхода: токенизацию по подсловам (__subword-based tokenization__).</small>

<br/>

3. __Токенизация на основе подслов (subword-based tokenization)__
Методы токенизации на основе подслов направлены на представление всех слов в наборе данных, используя только N токенов, где N — параметр, который определяется в соответствии с вашими требованиями (как правило он колеблется в районе ~30 000). Благодаря этому методу, без необходимости в огромном словаре, можно получить словарь из контекстно-независимых семантически насыщенных представлений слов с помощью подслов. Перед рабочим запуском программы для заполнения словаря нужна определенная тренировочная выборка. На этом этапе возможно применение различных алгоритмов:
    - __Кодирование пар байтов (англ. Byte-Pair Encoding, BPE)__: заполняет словарь токенами со значениями, формирующимися из слияния лексем, которые наиболее часто следуют подряд в тексте тренировочной выборки;
    - __WordPiece__: этот алгоритм похож на BPE, отличие только в том, что значения новых токенов образуются из слияния лексем, которые наиболее часто встречаются в словаре и никак не связано с их взаимным расположением в тексте тренировочной выборке;
    - __Unigram Language Model (ULM)__: при таком подходе сначала создается большой словарь (например, с помощью BPE), затем, пока размер словаря не достигнет заданного порога из него удаляются значения, вероятность встречи которых наименьшая (при этом одиночные символы из словаря не удаляются, а «вероятность» каждого значения вычисляется как произведение вероятностей подслов-значений, составивших его);
    - __SentencePiece__: в отличие от вышеперечисленных методов, не требует предварительной обработки текста перед сегментацией и обрабатывает пробелы как отдельный символ, что очень полезно для языков, где символ пробела не используется (например, китайского или японского). Другой главной особенностью SentencePiece является обратимая токенизация: поскольку в нем нет специальной обработки пробелов, декодирование токенов осуществляется просто путем их конкатенации;
    - и др.

_____
#### Источники:
+ [Wikipedia](https://ru.wikipedia.org/wiki/%D0%9B%D0%B5%D0%BA%D1%81%D0%B8%D1%87%D0%B5%D1%81%D0%BA%D0%B8%D0%B9_%D0%B0%D0%BD%D0%B0%D0%BB%D0%B8%D0%B7)

+ [Habr](https://habr.com/ru/articles/738176/)

+ [DataStart](https://datastart.ru/blog/read/plavnoe-vvedenie-v-natural-language-processing-nlp)

+ [towardsdatascience.com](https://towardsdatascience.com/tokenization-algorithms-explained-e25d5f4322ac)

+ [Hugging Face](https://huggingface.co/learn/nlp-course/ru/chapter2/4)

+ [alexanderdyakonov.com](https://alexanderdyakonov.wordpress.com/2019/11/29/%D1%82%D0%BE%D0%BA%D0%B5%D0%BD%D0%B8%D0%B7%D0%B0%D1%86%D0%B8%D1%8F-%D0%BD%D0%B0-%D0%BF%D0%BE%D0%B4%D1%81%D0%BB%D0%BE%D0%B2%D0%B0-subword-tokenization/)

+ [Youtube video](https://www.youtube.com/watch?v=hL4ZnAWSyuU)










